{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vVSSr8c_Mbri"
   },
   "source": [
    "### Libraries üìö‚¨á"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YwsBmqStMbrj",
    "outputId": "c8f05887-34a6-4484-c77c-9a96675c770c"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "env: SM_FRAMEWORK=tf.keras\nSegmentation Models: using `tf.keras` framework.\nFinished\n"
     ]
    }
   ],
   "source": [
    "import os, cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import albumentations as album\n",
    "import tensorflow as tf\n",
    "\n",
    "%env SM_FRAMEWORK=tf.keras\n",
    "import segmentation_models as sm\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s2vU3LKlMbrk"
   },
   "source": [
    "### Defining train / val / test directories üìÅ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oWGnW7PhMbrl"
   },
   "outputs": [],
   "source": [
    "# FOR COLAB\n",
    "\n",
    "# %env KAGGLE_USERNAME=pgrynfelder\n",
    "# %env KAGGLE_KEY=db39d1a3db70f8135b05b032937ce861\n",
    "\n",
    "# !kaggle datasets download -d balraj98/massachusetts-roads-dataset --force\n",
    "# !unzip -q massachusetts-roads-dataset.zip\n",
    "\n",
    "# print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y632r7VsNBw3"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# DATA_DIR = '../input/massachusetts-roads-dataset/tiff/'\n",
    "DATA_DIR = \"tiff\"\n",
    "x_train_dir = os.path.join(DATA_DIR, 'train')\n",
    "y_train_dir = os.path.join(DATA_DIR, 'train_labels')\n",
    "\n",
    "x_valid_dir = os.path.join(DATA_DIR, 'val')\n",
    "y_valid_dir = os.path.join(DATA_DIR, 'val_labels')\n",
    "\n",
    "x_test_dir = os.path.join(DATA_DIR, 'test')\n",
    "y_test_dir = os.path.join(DATA_DIR, 'test_labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z9RYv3VTMbrl",
    "outputId": "2c220be9-8951-4d72-cd97-cea2c94ad441"
   },
   "outputs": [],
   "source": [
    "# class_dict = pd.read_csv(\"../input/massachusetts-roads-dataset/label_class_dict.csv\")\n",
    "class_dict = pd.read_csv(\"label_class_dict.csv\")\n",
    "# Get class names\n",
    "class_names = class_dict['name'].tolist()\n",
    "# Get class RGB values\n",
    "class_rgb_values = class_dict[['r','g','b']].values.tolist()\n",
    "\n",
    "print('All dataset classes and their corresponding RGB values in labels:')\n",
    "print('Class Names: ', class_names)\n",
    "print('Class RGB values: ', class_rgb_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "67OYLASkMbrm"
   },
   "source": [
    "#### Shortlist specific classes to segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iZJdsmxUMbrm",
    "outputId": "c39cc415-3728-438c-9375-053a06050066"
   },
   "outputs": [],
   "source": [
    "# Useful to shortlist specific classes in datasets with large number of classes\n",
    "select_classes = ['background', 'road']\n",
    "\n",
    "# Get RGB values of required classes\n",
    "select_class_indices = [class_names.index(cls.lower()) for cls in select_classes]\n",
    "select_class_rgb_values =  np.array(class_rgb_values)[select_class_indices]\n",
    "\n",
    "print('Selected classes and their corresponding RGB values in labels:')\n",
    "print('Class Names: ', class_names)\n",
    "print('Class RGB values: ', class_rgb_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qbh_uROmMbrn"
   },
   "source": [
    "### Helper functions for viz. & one-hot encoding/decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AT4NP2j5Mbrn"
   },
   "outputs": [],
   "source": [
    "# helper function for data visualization\n",
    "def visualize(**images):\n",
    "    \"\"\"\n",
    "    Plot images in one row\n",
    "    \"\"\"\n",
    "    n_images = len(images)\n",
    "    plt.figure(figsize=(20,8))\n",
    "    for idx, (name, image) in enumerate(images.items()):\n",
    "        plt.subplot(1, n_images, idx + 1)\n",
    "        plt.xticks([]); \n",
    "        plt.yticks([])\n",
    "        # get title from the parameter names\n",
    "        plt.title(name.replace('_',' ').title(), fontsize=20)\n",
    "        plt.imshow(image)\n",
    "    plt.show()\n",
    "\n",
    "# Perform one hot encoding on label\n",
    "def one_hot_encode(label, label_values):\n",
    "    \"\"\"\n",
    "    Convert a segmentation image label array to one-hot format\n",
    "    by replacing each pixel value with a vector of length num_classes\n",
    "    # Arguments\n",
    "        label: The 2D array segmentation image label\n",
    "        label_values\n",
    "        \n",
    "    # Returns\n",
    "        A 2D array with the same width and hieght as the input, but\n",
    "        with a depth size of num_classes\n",
    "    \"\"\"\n",
    "    semantic_map = []\n",
    "    for colour in label_values:\n",
    "        equality = np.equal(label, colour)\n",
    "        class_map = np.all(equality, axis = -1)\n",
    "        semantic_map.append(class_map)\n",
    "    semantic_map = np.stack(semantic_map, axis=-1)\n",
    "\n",
    "    return semantic_map\n",
    "    \n",
    "# Perform reverse one-hot-encoding on labels / preds\n",
    "def reverse_one_hot(image):\n",
    "    \"\"\"\n",
    "    Transform a 2D array in one-hot format (depth is num_classes),\n",
    "    to a 2D array with only 1 channel, where each pixel value is\n",
    "    the classified class key.\n",
    "    # Arguments\n",
    "        image: The one-hot format image \n",
    "        \n",
    "    # Returns\n",
    "        A 2D array with the same width and hieght as the input, but\n",
    "        with a depth size of 1, where each pixel value is the classified \n",
    "        class key.\n",
    "    \"\"\"\n",
    "    x = np.argmax(image, axis = -1)\n",
    "    return x\n",
    "\n",
    "# Perform colour coding on the reverse-one-hot outputs\n",
    "def colour_code_segmentation(image, label_values):\n",
    "    \"\"\"\n",
    "    Given a 1-channel array of class keys, colour code the segmentation results.\n",
    "    # Arguments\n",
    "        image: single channel array where each value represents the class key.\n",
    "        label_values\n",
    "\n",
    "    # Returns\n",
    "        Colour coded image for segmentation visualization\n",
    "    \"\"\"\n",
    "    colour_codes = np.array(label_values)\n",
    "    x = colour_codes[image.astype(int)]\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RcrfQvf0Mbro"
   },
   "outputs": [],
   "source": [
    "class RoadsDataset(tf.keras.utils.Sequence):\n",
    "\n",
    "    \"\"\"Massachusetts Roads Dataset. Read images, apply augmentation and preprocessing transformations.\n",
    "    \n",
    "    Args:\n",
    "        images_dir (str): path to images folder\n",
    "        masks_dir (str): path to segmentation masks folder\n",
    "        class_rgb_values (list): RGB values of select classes to extract from segmentation mask\n",
    "        augmentation (albumentations.Compose): data transfromation pipeline \n",
    "            (e.g. flip, scale, etc.)\n",
    "        preprocessing (albumentations.Compose): data preprocessing \n",
    "            (e.g. noralization, shape manipulation, etc.)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "            self, \n",
    "            batch_size,\n",
    "            images_dir, \n",
    "            masks_dir, \n",
    "            class_rgb_values=None, \n",
    "            augmentation=None, \n",
    "            preprocessing=None,\n",
    "    ):\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.image_paths = [os.path.join(images_dir, image_id) for image_id in sorted(os.listdir(images_dir))]\n",
    "        self.mask_paths = [os.path.join(masks_dir, image_id) for image_id in sorted(os.listdir(masks_dir))]\n",
    "        \n",
    "        self.class_rgb_values = class_rgb_values\n",
    "        self.augmentation = augmentation\n",
    "        self.preprocessing = preprocessing\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths) // self.batch_size\n",
    "    \n",
    "    \n",
    "    def __getsingle(self, i):\n",
    "        \n",
    "        # read images and masks\n",
    "        image = cv2.cvtColor(cv2.imread(self.image_paths[i]), cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.cvtColor(cv2.imread(self.mask_paths[i]), cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # one-hot-encode the mask\n",
    "        mask = one_hot_encode(mask, self.class_rgb_values).astype('float')\n",
    "        \n",
    "        # apply augmentations\n",
    "        if self.augmentation:\n",
    "            sample = self.augmentation(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "        \n",
    "        # apply preprocessing\n",
    "        if self.preprocessing:\n",
    "            sample = self.preprocessing(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "            \n",
    "        return image, mask\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        indices = range(self.batch_size*i, min(len(self.image_paths), self.batch_size*(i+1)))\n",
    "        X, Y = [], []\n",
    "        for j in indices:\n",
    "            x, y = self.__getsingle(j)\n",
    "            X.append(x)\n",
    "            Y.append(y)\n",
    "        return np.array(X), np.array(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DvQrtW30Mbrr"
   },
   "source": [
    "#### Visualize Sample Image and Mask üìà"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 378
    },
    "id": "2KrZFpbbMbrt",
    "outputId": "e68c4d6f-698f-4614-d747-799e700268ad"
   },
   "outputs": [],
   "source": [
    "dataset = RoadsDataset(32, x_train_dir, y_train_dir, class_rgb_values=select_class_rgb_values)\n",
    "\n",
    "\n",
    "images, masks = dataset[2]\n",
    "\n",
    "image, mask = images[0], masks[0]\n",
    "\n",
    "visualize(\n",
    "    original_image = image,\n",
    "    ground_truth_mask = colour_code_segmentation(reverse_one_hot(mask), select_class_rgb_values),\n",
    "    one_hot_encoded_mask = reverse_one_hot(mask)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GHBcILAPMbru"
   },
   "source": [
    "### Defining Augmentations üôÉ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bWJKpHZsMbru"
   },
   "outputs": [],
   "source": [
    "def get_training_augmentation():\n",
    "    train_transform = [    \n",
    "        album.RandomCrop(height=256, width=256, always_apply=True),\n",
    "        album.OneOf(\n",
    "            [\n",
    "                album.HorizontalFlip(p=1),\n",
    "                album.VerticalFlip(p=1),\n",
    "                album.RandomRotate90(p=1),\n",
    "            ],\n",
    "            p=0.75,\n",
    "        ),\n",
    "    ]\n",
    "    return album.Compose(train_transform)\n",
    "\n",
    "\n",
    "def get_validation_augmentation():   \n",
    "    # Add sufficient padding to ensure image is divisible by 32\n",
    "    test_transform = [\n",
    "        album.PadIfNeeded(min_height=1536, min_width=1536, always_apply=True, border_mode=0),\n",
    "    ]\n",
    "    return album.Compose(test_transform)\n",
    "\n",
    "\n",
    "def to_tensor(x, **kwargs):\n",
    "    return x.transpose(2, 0, 1).astype('float32')\n",
    "\n",
    "\n",
    "def get_preprocessing(preprocessing_fn=None):\n",
    "    \"\"\"Construct preprocessing transform    \n",
    "    Args:\n",
    "        preprocessing_fn (callable): data normalization function \n",
    "            (can be specific for each pretrained neural network)\n",
    "    Return:\n",
    "        transform: albumentations.Compose\n",
    "    \"\"\"   \n",
    "    _transform = []\n",
    "    if preprocessing_fn:\n",
    "        _transform.append(album.Lambda(image=preprocessing_fn))\n",
    "    # _transform.append(album.Lambda(image=to_tensor, mask=to_tensor))\n",
    "        \n",
    "    return album.Compose(_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lEz0xVZoMbru"
   },
   "source": [
    "#### Visualize Augmented Images & Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "5gEIGugKMbrv",
    "outputId": "aa0d7f30-2a06-4fac-c84f-9954bd68bebf"
   },
   "outputs": [],
   "source": [
    "augmented_dataset = RoadsDataset(\n",
    "    32,\n",
    "    x_train_dir, y_train_dir, \n",
    "    augmentation=get_training_augmentation(),\n",
    "    class_rgb_values=select_class_rgb_values,\n",
    ")\n",
    "\n",
    "random_idx = random.randint(0, len(augmented_dataset)-1)\n",
    "\n",
    "# Different augmentations on a random image/mask pair (256*256 crop)\n",
    "images, masks = augmented_dataset[5]\n",
    "image, mask = images[0], masks[0]\n",
    "visualize(\n",
    "    original_image = image,\n",
    "    ground_truth_mask = colour_code_segmentation(reverse_one_hot(mask), select_class_rgb_values),\n",
    "    one_hot_encoded_mask = reverse_one_hot(mask)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MaNgrY2X_kTW"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mXgsnTQsMbrv"
   },
   "outputs": [],
   "source": [
    "# Get train and val dataset instances\n",
    "train_dataset = RoadsDataset(\n",
    "    32,\n",
    "    x_train_dir, y_train_dir, \n",
    "    augmentation=get_training_augmentation(),\n",
    "    preprocessing=get_preprocessing(),\n",
    "    class_rgb_values=select_class_rgb_values,\n",
    ")\n",
    "\n",
    "valid_dataset = RoadsDataset(\n",
    "    1,\n",
    "    x_valid_dir, y_valid_dir, \n",
    "    augmentation=get_validation_augmentation(), \n",
    "    preprocessing=get_preprocessing(),\n",
    "    class_rgb_values=select_class_rgb_values,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "-zd15DP6Mbrv"
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-14c9e6f4cfca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mBACKBONE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"resnet50\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpreprocess_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_preprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBACKBONE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "BACKBONE = \"resnet50\"\n",
    "preprocess_input = sm.get_preprocessing(BACKBONE)\n",
    "train_dataset = preprocess_input(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "nxgasFroMbrv"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading data from https://github.com/qubvel/classification_models/releases/download/0.0.1/resnet50_imagenet_1000_no_top.h5\n",
      "94593024/94592056 [==============================] - 18s 0us/step\n"
     ]
    }
   ],
   "source": [
    "model = sm.Unet(BACKBONE, classes=2)\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=\"epoch{epoch:02d}-loss{val_loss:.4f}.h5\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RIsgN3S1SFzV",
    "outputId": "ae869365-de8b-435f-c9ce-1231d6820fc9"
   },
   "outputs": [],
   "source": [
    "model.compile('Adam', loss=sm.losses.bce_jaccard_loss, metrics=['accuracy']) # bce jaccard == IOU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_dataset, epochs=10, callbacks=[model_checkpoint_callback], validation_data=valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model/assets\n"
     ]
    }
   ],
   "source": [
    "model.load_weights(\"epoch18-iou0.7055.h5\")\n",
    "model.save(\"saved_model\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "deepsat-road-tf.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}